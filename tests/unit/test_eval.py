# SPDX-License-Identifier: MPL-2.0

import pytest

from msk_io.errors import DataValidationError
from msk_io.eval.evaluator import Evaluator
from msk_io.schema.image_analysis import (
    ImageMetaData,
    ImageSegmentationResult,
    RegionOfInterest,
)
from msk_io.schema.llm_output import (
    DiagnosticFinding,
    LLMAnalysisResult,
    LLMOutput,
    LLMResponseChoice,
)
from msk_io.schema.metrics import EvaluationReport


@pytest.fixture
def evaluator_instance(test_config):
    return Evaluator(test_config)


@pytest.fixture
def mock_segmentation_result(mock_dicom_volume) -> ImageSegmentationResult:
    return ImageSegmentationResult(
        source_volume=mock_dicom_volume,
        segmentation_id="seg_test_id",
        regions_of_interest=[
            RegionOfInterest(roi_id="roi_1", label="Tumor", pixel_count=1000),
            RegionOfInterest(roi_id="roi_2", label="Organ", pixel_count=5000),
        ],
        segmentation_method="DL-UNet",
        processed_image_meta=ImageMetaData(
            original_path=mock_dicom_volume.volume_path,
            image_format="nifti",
            dimensions=[100, 100, 100],
        ),
    )


@pytest.fixture
def mock_llm_analysis_result() -> LLMAnalysisResult:
    return LLMAnalysisResult(
        llm_output=LLMOutput(
            response_id="llm_resp_test",
            model_name_used="gpt-4o",
            input_tokens=50,
            output_tokens=100,
            total_tokens=150,
            choices=[
                LLMResponseChoice(
                    index=0, text="Analysis complete. Finding: Anomaly detected."
                )
            ],
        ),
        extracted_findings=[
            DiagnosticFinding(
                category="Anomaly",
                description="Lesion detected.",
                severity="MEDIUM",
                confidence_score=0.8,
            ),
            DiagnosticFinding(
                category="Pathology",
                description="Inflammation suspected.",
                severity="LOW",
                confidence_score=0.6,
            ),
        ],
        summary="Summary of findings from LLM.",
    )


@pytest.mark.asyncio
async def test_evaluate_segmentation_success(
    evaluator_instance, mock_segmentation_result
):
    ground_truth = [
        {"roi_id": "gt_roi_1", "mask_path": "/path/to/gt_mask.nii.gz", "label": "Tumor"}
    ]
    report = await evaluator_instance.evaluate_segmentation(
        ground_truth, mock_segmentation_result
    )
    assert isinstance(report, EvaluationReport)
    assert report.evaluation_target == "Segmentation Model"
    assert report.evaluated_entity_id == "seg_test_id"
    assert report.status == "PASSED"
    assert any(m.name == "Dice Coefficient (Simulated)" for m in report.metrics)


@pytest.mark.asyncio
async def test_evaluate_segmentation_no_ground_truth(
    evaluator_instance, mock_segmentation_result
):
    with pytest.raises(DataValidationError):
        await evaluator_instance.evaluate_segmentation([], mock_segmentation_result)


@pytest.mark.asyncio
async def test_evaluate_llm_analysis_success(
    evaluator_instance, mock_llm_analysis_result
):
    ground_truth = [
        DiagnosticFinding(
            category="Anomaly",
            description="Lesion detected.",
            severity="MEDIUM",
            confidence_score=0.9,
        )
    ]
    report = await evaluator_instance.evaluate_llm_analysis(
        ground_truth, mock_llm_analysis_result
    )
    assert isinstance(report, EvaluationReport)
    assert report.evaluation_target == "LLM Inference Agent"
    assert report.status in {"PASSED", "FAILED"}


@pytest.mark.asyncio
async def test_evaluate_llm_analysis_no_ground_truth(
    evaluator_instance, mock_llm_analysis_result
):
    with pytest.raises(DataValidationError):
        await evaluator_instance.evaluate_llm_analysis([], mock_llm_analysis_result)
