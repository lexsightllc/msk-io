# SPDX-License-Identifier: MPL-2.0
from typing import Any

from msk_io.errors import DataValidationError
from msk_io.schema.image_analysis import ImageSegmentationResult
from msk_io.schema.llm_output import DiagnosticFinding, LLMAnalysisResult
from msk_io.schema.metrics import EvaluationReport, Metric
from msk_io.utils.decorators import handle_errors, log_method_entry_exit
from msk_io.utils.log_config import get_logger

logger = get_logger(__name__)


class Evaluator:
    def __init__(self, config):
        self.config = config
        logger.info("Evaluator initialized.")

    @handle_errors
    @log_method_entry_exit
    def evaluate_segmentation(
        self,
        ground_truth_masks: list[dict[str, Any]],
        predicted_segmentation: ImageSegmentationResult,
    ) -> EvaluationReport:
        logger.warning(
            "Evaluator.evaluate_segmentation is a conceptual stub. Returning dummy metrics."
        )
        if not ground_truth_masks:
            raise DataValidationError(
                "Ground truth masks are required for segmentation evaluation."
            )
        if not predicted_segmentation.regions_of_interest:
            logger.warning(
                "Predicted segmentation has no ROIs. Evaluation will be limited."
            )
        metrics = []
        dummy_dice_score = 0.85 if predicted_segmentation.regions_of_interest else 0.0
        metrics.append(
            Metric(
                name="Dice Coefficient (Simulated)",
                value=dummy_dice_score,
                unit="ratio",
                description="Similarity between predicted and ground truth masks.",
            )
        )
        metrics.append(
            Metric(
                name="Jaccard Index (Simulated)",
                value=dummy_dice_score * 0.9,
                unit="ratio",
            )
        )
        status = "PASSED" if dummy_dice_score > 0.7 else "FAILED"
        return EvaluationReport(
            evaluation_target="Segmentation Model",
            evaluated_entity_id=predicted_segmentation.segmentation_id,
            metrics=metrics,
            dataset_info={"ground_truth_count": len(ground_truth_masks)},
            qualitative_observations="Simulated segmentation evaluation. Performance depends on dummy data.",
            status=status,
        )

    @handle_errors
    @log_method_entry_exit
    def evaluate_llm_analysis(
        self,
        ground_truth_findings: list[DiagnosticFinding],
        predicted_llm_analysis: LLMAnalysisResult,
    ) -> EvaluationReport:
        logger.warning(
            "Evaluator.evaluate_llm_analysis is a conceptual stub. Returning dummy metrics."
        )
        if not ground_truth_findings:
            raise DataValidationError(
                "Ground truth findings are required for LLM analysis evaluation."
            )
        if not predicted_llm_analysis.extracted_findings:
            logger.warning(
                "LLM analysis has no extracted findings. Evaluation will be limited."
            )
        metrics = []
        num_matched_findings = 0
        for gt_f in ground_truth_findings:
            for pred_f in predicted_llm_analysis.extracted_findings:
                if gt_f.category == pred_f.category and pred_f.confidence_score > 0.7:
                    num_matched_findings += 1
                    break
        dummy_precision = (
            num_matched_findings / len(predicted_llm_analysis.extracted_findings)
            if predicted_llm_analysis.extracted_findings
            else 0.0
        )
        dummy_recall = (
            num_matched_findings / len(ground_truth_findings)
            if ground_truth_findings
            else 0.0
        )
        metrics.append(
            Metric(
                name="Finding Precision (Simulated)",
                value=dummy_precision,
                unit="ratio",
            )
        )
        metrics.append(
            Metric(name="Finding Recall (Simulated)", value=dummy_recall, unit="ratio")
        )
        if dummy_precision + dummy_recall > 0:
            metrics.append(
                Metric(
                    name="F1 Score (Simulated)",
                    value=2
                    * (dummy_precision * dummy_recall)
                    / (dummy_precision + dummy_recall),
                    unit="ratio",
                )
            )
        else:
            metrics.append(Metric(name="F1 Score (Simulated)", value=0.0, unit="ratio"))
        status = "PASSED" if dummy_precision > 0.6 and dummy_recall > 0.6 else "FAILED"
        return EvaluationReport(
            evaluation_target="LLM Inference Agent",
            evaluated_entity_id=predicted_llm_analysis.analysis_id,
            metrics=metrics,
            dataset_info={"ground_truth_finding_count": len(ground_truth_findings)},
            qualitative_observations="Simulated LLM analysis evaluation. Focus on conceptual match.",
            status=status,
        )
