# SPDX-License-Identifier: MPL-2.0
import os
from typing import Dict, Any, List, Optional, Type
from datetime import datetime
from uuid import uuid4
from msk_io.schema.llm_output import LLMInput, LLMOutput, LLMResponseChoice, LLMAnalysisResult, DiagnosticFinding
from msk_io.schema.image_analysis import ImageAnalysisResult
from msk_io.schema.dicom_data import DICOMPatientInfo
from msk_io.schema.prompt_template import PromptTemplate
from msk_io.errors import LLMInferenceError, ConfigurationError, ExternalServiceError
from msk_io.utils.log_config import get_logger
from msk_io.utils.decorators import handle_errors, log_method_entry_exit, requires_config

logger = get_logger(__name__)

class BaseLLMAgent:
    def __init__(self, config):
        self.config = config
        self.model_name = config.llm.default_llm_model
        self.timeout = config.llm.llm_timeout_seconds
        self._client: Any = None

    @handle_errors
    @log_method_entry_exit
    def _initialize_client(self) -> Any:
        raise NotImplementedError("Subclasses must implement _initialize_client method.")

    @handle_errors
    @log_method_entry_exit
    def _call_llm_api(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:
        raise NotImplementedError("Subclasses must implement _call_llm_api method.")

    @handle_errors
    @log_method_entry_exit
    def analyze_data(self, prompt_template: PromptTemplate, context_data: Dict[str, Any]) -> LLMAnalysisResult:
        logger.warning(f"{self.__class__.__name__}.analyze_data is a conceptual stub. Simulating LLM analysis.")
        if self._client is None:
            self._client = self._initialize_client()
        try:
            formatted_prompt = prompt_template.format(**context_data)
            logger.debug(f"Formatted LLM prompt: {formatted_prompt[:200]}...")
            simulated_raw_response = {
                "id": f"chatcmpl-{uuid4()}",
                "object": "chat.completion",
                "created": int(datetime.now().timestamp()),
                "model": self.model_name,
                "usage": {"prompt_tokens": len(formatted_prompt.split()), "completion_tokens": 100, "total_tokens": len(formatted_prompt.split()) + 100},
                "choices": [
                    {
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": f"Based on the provided data, the patient has a simulated condition. {{'diagnosis': 'Simulated Anomaly', 'severity': 'MEDIUM', 'confidence': 0.85}}. Further review recommended."
                        },
                        "logprobs": None,
                        "finish_reason": "stop"
                    }
                ]
            }
            logger.info(f"Simulated LLM response from {self.model_name}.")
            llm_output = LLMOutput(
                response_id=simulated_raw_response['id'],
                model_name_used=simulated_raw_response['model'],
                input_tokens=simulated_raw_response['usage']['prompt_tokens'],
                output_tokens=simulated_raw_response['usage']['completion_tokens'],
                total_tokens=simulated_raw_response['usage']['total_tokens'],
                choices=[
                    LLMResponseChoice(index=c['index'], text=c['message']['content'], finish_reason=c['finish_reason'])
                    for c in simulated_raw_response['choices']
                ],
                raw_response=simulated_raw_response
            )
            extracted_findings: List[DiagnosticFinding] = []
            if "Simulated Anomaly" in llm_output.choices[0].text:
                extracted_findings.append(DiagnosticFinding(
                    category="Anomaly",
                    description="Simulated Anomaly detected based on combined data.",
                    severity="MEDIUM",
                    confidence_score=0.85,
                    associated_roi_ids=[],
                    supporting_evidence_texts=["simulated finding based on data"]
                ))
            summary_text = llm_output.choices[0].text.split("The findings suggest:")[0].strip()
            return LLMAnalysisResult(
                llm_output=llm_output,
                extracted_findings=extracted_findings,
                summary=summary_text
            )
        except Exception as e:
            raise LLMInferenceError(f"Failed to analyze data with LLM {self.model_name}: {e}") from e

class MiniGPTAgent(BaseLLMAgent):
    def _initialize_client(self) -> Any:
        logger.info(f"Initializing MiniGPT agent with model: {self.model_name}")
        return "MiniGPT_Client_Instance"

    def _call_llm_api(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:
        logger.warning("MiniGPTAgent._call_llm_api is a conceptual stub. Returning dummy response.")
        return {"text": f"MiniGPT says: {prompt[:50]}... (Simulated small model response)", "tokens_used": len(prompt.split()) + 50, "raw_output": {"model": self.model_name}}

class GEMAAgent(BaseLLMAgent):
    @requires_config("llm.google_api_key")
    def _initialize_client(self) -> Any:
        logger.info(f"Initializing GEMA agent with model: {self.model_name}")
        return "GEMA_Client_Instance"

    def _call_llm_api(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:
        logger.warning("GEMAAgent._call_llm_api is a conceptual stub. Returning dummy response.")
        return {
            "candidates": [{"content": {"parts": [{"text": f"GEMA says: {prompt[:50]}... (Simulated Google model response)"}]}}],
            "usage_metadata": {"prompt_token_count": len(prompt.split()), "total_token_count": len(prompt.split()) + 120},
            "model": self.model_name
        }

class PHI2Agent(BaseLLMAgent):
    def _initialize_client(self) -> Any:
        logger.info(f"Initializing PHI-2 agent with model: {self.model_name}")
        return "PHI2_Client_Instance"

    def _call_llm_api(self, prompt: str, **kwargs: Any) -> Dict[str, Any]:
        logger.warning("PHI2Agent._call_llm_api is a conceptual stub. Returning dummy response.")
        return {"generated_text": f"PHI-2 says: {prompt[:50]}... (Simulated local model response)"}

class LLMAgentFactory:
    AGENT_MAP: Dict[str, Type[BaseLLMAgent]] = {
        "minigpt": MiniGPTAgent,
        "gema": GEMAAgent,
        "phi2": PHI2Agent,
    }

    @classmethod
    @handle_errors
    def get_agent(cls, agent_type: str, config) -> BaseLLMAgent:
        agent_class = cls.AGENT_MAP.get(agent_type.lower())
        if not agent_class:
            raise ConfigurationError(f"Unknown LLM agent type: {agent_type}. Available types: {list(cls.AGENT_MAP.keys())}")
        logger.info(f"Creating LLM agent of type: {agent_type}")
        return agent_class(config)
